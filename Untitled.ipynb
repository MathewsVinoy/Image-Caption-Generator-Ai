{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954c54f1-0f15-4df7-b25e-19a03f2db75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af88ffe-09f6-4572-baea-42e068192d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f592249e-23f4-4136-99ff-eb331482647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"results.csv\")\n",
    "image_path = \"data/\"\n",
    "data = pd.read_csv(csv_path, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154400e4-0db3-4171-abe2-6d610ad94f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Workers look down from up above on a piece of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men working on a machine wearing hard hats .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name  comment_number  \\\n",
       "0  1000092795.jpg               0   \n",
       "1  1000092795.jpg               1   \n",
       "2  1000092795.jpg               2   \n",
       "3  1000092795.jpg               3   \n",
       "4  1000092795.jpg               4   \n",
       "5    10002456.jpg               0   \n",
       "6    10002456.jpg               1   \n",
       "7    10002456.jpg               2   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  \n",
       "5   Several men in hard hats are operating a gian...  \n",
       "6   Workers look down from up above on a piece of...  \n",
       "7   Two men working on a machine wearing hard hats .  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d42aa86-3eb1-4cb0-a101-1546c75fdda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_name         object\n",
       " comment_number    object\n",
       " comment           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84904e3a-8394-40d5-bfaa-770dc925452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158915"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c661214-a67e-4ca9-a9c5-2867561b027f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000092795.jpg\n",
       "1    1000092795.jpg\n",
       "2    1000092795.jpg\n",
       "3    1000092795.jpg\n",
       "4    1000092795.jpg\n",
       "5      10002456.jpg\n",
       "6      10002456.jpg\n",
       "7      10002456.jpg\n",
       "Name: image_name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_name'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08eec4c7-9d29-4d87-b735-949385ca3836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Two young guys with shaggy hair look at their...\n",
       "1     Two young , White males are outside near many...\n",
       "2     Two men in green shirts are standing in a yard .\n",
       "3         A man in a blue shirt standing in a garden .\n",
       "4              Two friends enjoy time spent together .\n",
       "5     Several men in hard hats are operating a gian...\n",
       "6     Workers look down from up above on a piece of...\n",
       "7     Two men working on a machine wearing hard hats .\n",
       "Name:  comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[' comment'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1deb3fb-47d9-4fee-9ff7-e4426c290ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open(f\"{image_path}{data['image_name'][1000]}\") as im:\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e79337-797f-41fc-bbdf-2182aba0f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data[' comment'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08c2334-136a-4d95-97b4-089e580d5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_tokens = []\n",
    "for text in data[' comment'].astype(str).fillna(''):\n",
    "    tokens = word_tokenize(text)\n",
    "    all_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9a884f1-84f5-46b3-917f-b9c17a476113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23436"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(set(all_tokens))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4072248-f6c7-4d95-bebe-db84adf6834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11780,  4408, 16051, ..., 15237,  3035, 20969])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(tokens)\n",
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39cfd949-7b0f-4fdc-8d16-0091b24b0954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gemmed', 'SquarePants', 'ox', ..., 'mounts', 'Michael', 'swarm'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens = label_encoder.inverse_transform(integer_encoded)\n",
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d2dd715-d453-4006-9b81-80706a00bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_num = {char: idx for idx, char in enumerate(set(tokens))}\n",
    "num_to_words = {idx: char for idx, char in enumerate(set(tokens))}\n",
    "\n",
    "encode = lambda s: [words_to_num[c] for c in word_tokenize(s)]\n",
    "def encode2(l):\n",
    "\n",
    "    l=str(l)\n",
    "    value = [words_to_num[c] for c in word_tokenize(l)]\n",
    "    while len(value) <82:\n",
    "        value.append(0)\n",
    "    return torch.tensor(value, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "def decoder(l):\n",
    "    return ' '.join([num_to_words.get(i,'<UNK>') for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9125b7c0-cb3d-4305-9970-9958731b266b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "for i in data[' comment']:\n",
    "    # print(\"String:->\",i)\n",
    "    v=encode2(i)\n",
    "    if n < len(v):\n",
    "        n=len(v)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b0e44b-d106-482c-a449-c368fbf41226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10469.,  9724.,  3290.,  8510., 22589., 21234.,  4342., 15516., 13888.,\n",
       "        21463., 22996.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode2(data[' comment'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b3fefe-4526-4ce1-9f70-2aa31dc99116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['image_name'], data[' comment'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df0bda9-9757-48bc-b22c-09c8690a780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1000092795.jpg',\n",
       " ' Two young guys with shaggy hair look at their hands while hanging out in the yard .')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d394fdbf-f11d-4478-9b28-e984c20e5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetsCustom(Dataset):\n",
    "    def __init__(self, x, y, img_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.X = x.reset_index(drop=True)  # Reset index to avoid indexing issues\n",
    "        self.y = y.reset_index(drop=True)  # Reset index to avoid indexing issues\n",
    "        self.img_path = img_path\n",
    "\n",
    "    def load_image(self, idx):\n",
    "        image_path = f\"{self.img_path}/{self.X.iloc[idx]}\"\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "        return imagez\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        output = self.y.iloc[idx]\n",
    "        output = encode2(output)\n",
    "        # output = torch.tensor(output, dtype=torch.float32)  # Ensure output is a tensor\n",
    "        return img, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83889066-8903-4195-acff-4b961e0ede02",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.RandomRotation(degrees=45),  \n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_datasets = DatasetsCustom(x=X_train,y=y_train,img_path=image_path,transform=transform1)\n",
    "test_datasets = DatasetsCustom(x=X_test,y=y_test,img_path=image_path,transform=transform1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11bb2d2a-610b-4f89-8b60-835c347f943d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143023"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61cee7d1-c591-40cd-8045-d4a02bf59ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_datasets, batch_size=32, num_workers=0, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_datasets, batch_size=32, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce944bd-f638-41ca-8367-e67032eebd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x70f9ac04b350>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf26b46-11e4-44a1-9aa2-a4995a71d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the fully connected layer\n",
    "        self.lstm = nn.LSTM(input_size=input_shape, hidden_size=hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_shape)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size = x.size(0)\n",
    "        features = self.resnet(x)  # Extract features using ResNet\n",
    "        features = features.view(batch_size, 1, -1)  # Reshape for LSTM input\n",
    "        lstm_out, (h_n, c_n) = self.lstm(features)\n",
    "        lstm_out_last = lstm_out[:, -1, :]  # Get the last output of the LSTM\n",
    "        output = self.fc(lstm_out_last)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4982ef9b-0497-48e8-a0b5-5abc9e55742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathewsvinoy/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mathewsvinoy/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model=Model(input_shape=512,output_shape=82,hidden_units=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6471e-a18a-4467-bdfb-2f19bb7f3afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4470/4470 [6:41:31<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 552590.890072707, test loss 546420.9652288732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████▍                | 2354/4470 [3:30:07<3:30:39,  5.97s/it]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, train loss {train_loss / len(train_dataloader)}, test loss {test_loss / len(test_dataloader)}\")\n",
    "        \n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_NAME = \"model_train.pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    \n",
    "    \n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    #Save\n",
    "    torch.save(obj=model.state_dict(),f=MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d8306-2c17-4435-a940-97e5d331bceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
